from stable_baselines3 import PPO


class PPOModified(PPO):

    def __init__(self,
                policy,
                env,
                learning_rate=3e-4,
                n_steps=500,
                batch_size=64,
                n_epochs=10,
                gamma=0.99,
                gae_lambda=0.95,
                clip_range=0.2,
                clip_range_vf=None,
                ent_coef=0.0,
                vf_coef=0.5,
                max_grad_norm=0.5,
                use_sde=False,
                sde_sample_freq=-1,
                target_kl=None,
                tensorboard_log=None,
                create_eval_env=False,
                policy_kwargs=None,
                verbose=0,
                seed=None,
                device="auto",
                _init_setup_model=True,
                **kwargs):

        PPO.__init__(self,
                     policy,
                     env,
                     learning_rate,
                     n_steps,
                     batch_size,
                     n_epochs,
                     gamma,
                     gae_lambda,
                     clip_range,
                     clip_range_vf,
                     ent_coef,
                     vf_coef,
                     max_grad_norm,
                     use_sde,
                     sde_sample_freq,
                     target_kl,
                     tensorboard_log,
                     create_eval_env,
                     policy_kwargs,
                     verbose,
                     seed,
                     device,
                     _init_setup_model
                     )
